---
layout: post-no-feature
title: "NADEs: an introduction"
description: "A summary of my understanding of NADEs"
comments: true
categories: articles
date: 2014-02-25
published: true
---

I might use neural autoregressive distribution estimators (NADEs) for the speech
synthesis project; this has to do with an idea both Guillaume Desjardins and
Yoshua Bengio talked about in the past couple days, and which I'll detail later
on. For now, I'd like to test my understanding of NADEs by introducing them in a
blog post. As they say,

> If you want to learn something, read. If you want to understand something,
> write. If you want to master something, teach.

### The idea

RBMs are able to model complex distributions and work very well as generative
models, but they're not well suited for density estimation because they present
an intractable partition function:
\\[
  p\_{RBM}(\mathbf{v}) = \sum\_{\mathbf{h}} p(\mathbf{v}, \mathbf{h})
                       = \sum\_{\mathbf{h}} \frac{
                             \exp(-E(\mathbf{v}, \mathbf{h}))
                         }{
                             \sum\_{\tilde{\mathbf{v}}, \tilde{\mathbf{h}}}
                             \exp(-E(\tilde{\mathbf{v}}, \tilde{\mathbf{h}}))
                         }
                       = \sum\_{\mathbf{h}} \frac{\exp(-E(\mathbf{v}, \mathbf{h}))}{Z}
\\]
We see that \\(Z\\) is intractable because it contains a number of terms
that's exponential in the dimensionality of \\(\mathbf{v}\\) and
\\(\mathbf{h}\\).

NADE ([original
paper](http://jmlr.org/proceedings/papers/v15/larochelle11a/larochelle11a.pdf))
is a model proposed by [Hugo Larochelle](http://www.dmi.usherb.ca/~larocheh/index_en.html)
and [Iain Murray](http://homepages.inf.ed.ac.uk/imurray2/) as a way to
circumvent this difficulty by decomposing the joint distribution
\\(p(\mathbf{v})\\) into tractable conditional distributions. It is inspired by
an attempt to convert an RBM into a Bayesian network.

![NADE]({{ site.url }}/images/nade.png)

The joint probability distribution \\(p(\mathbf{v})\\) over observed variables
is expressed as
\\[
    p(\mathbf{v}) = \prod\_{i=1}^D p(v\_i \mid \mathbf{v}\_{<i})
\\]
where
\\[
\begin{split}
    p(v\_i \mid \mathbf{v}\_{<i}) &=
        \text{sigm}(b\_i + \mathbf{V}\_{i}\cdot\mathbf{h}\_i), \\\\\
    \mathbf{h}\_i &=
        \text{sigm}(\mathbf{c} + \mathbf{W}\_{<i}\cdot\mathbf{v}\_{<i})
\end{split}
\\]

As you can see both in the graph and in the joint probability, given a specific
ordering, each observed variable only depends on prior variables in the
ordering. By abusing notation a little, we can consider \\(\mathbf{h}\_i\\) to
be a random vector whose conditional distribution is
\\(
    p(\mathbf{h}\_i \mid \mathbf{v}\_{<i})
    = \delta(\text{sigm}(\mathbf{c} + \mathbf{W}\_{<i}\cdot\mathbf{v}\_{<i}))
\\).

The distribution modeled by NADEs has the great advantage to be tractable, since
all of its conditional probability distributions are themselves tractable. This
means contrary to an RBM, performance can be directly measured via the negative
log-likelihood (NLL) of the dataset.

In (Larochelle & Murray, 2011), NADEs are shown to outperform common models with
tractable distributions and to have a performance comparable to large
intractable RBMs.

### Implementation and results

I ported JÃ¶rg Bornschein's NADE Theano implementation to Pylearn2 and used it to
reproduce Larochelle & Murray's results on MNIST. I intend on making a pull
request out of it so it's integrated in Pylearn2.

The trained model scores a __-85.8 test log-likelihood__, which is slightly
better than what is reported in the paper. To be fair, I made a mistake while
training and binarized training examples every time they were presented by
sampling from a Bernoulli distribution, which explains the better results.

Below are samples taken from the trained model (more precisely parameters of the
Bernoulli distributions that were output before the actual pixels were sampled)
and weights filters.

![NADE samples]({{ site.url }}/images/nade_samples.png)

![NADE filters]({{ site.url }}/images/nade_filters.png)
